---
title: 'Project 2: Modeling, Testing, and Predicting'
author: " Reem Ayoub ra35556 "
date: ''
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
---



<div id="modeling" class="section level1">
<h1>Modeling</h1>
<div id="introduction" class="section level2">
<h2>Introduction:</h2>
<p>For this project I decided to look at a data set of the population of Pima Indians in Arizona, and their diagnosis as diabetic or not. The data set includes the variables number of pregnancies (npreg), plasma glucose concentration (glu), diastolic blood pressure (bp), triceps skin fold thickness (skin), body mass index (bmi), diabetes pedigree function (ped), age and the classification as diabetic or not.</p>
<pre class="r"><code>diab &lt;-read.csv(&quot;Pima.tr.csv&quot;)
diab&lt;-diab%&gt;%dplyr::select(-`X`)</code></pre>
</div>
<div id="manova" class="section level2">
<h2>MANOVA</h2>
<p>Briefly discuss assumptions and whether or not they are likely to have been met (2).</p>
<pre class="r"><code>man1&lt;-manova(cbind(npreg,glu,bp,skin,bmi,age,ped)~type, data=diab)
summary(man1)</code></pre>
<pre><code>## Df Pillai approx F num Df den Df Pr(&gt;F)
## type 1 0.34346 14.349 7 192 5.919e-15 ***
## Residuals 198
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<p>A one-way MANOVA test was conducted to determine the effect of diabetes on the variables npreg, glu, bp, skin, bmi, age and ped. After completing a MANOVA, it was found that at least one of the numeric variables showed a mean difference between diabetic and non diabetic women. This was supported by the calculation of a p value less than 0.05.</p>
<pre class="r"><code>summary.aov(man1)</code></pre>
<pre><code>## Response npreg :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## type 1 165.72 165.716 15.705 0.0001033 ***
## Residuals 198 2089.30 10.552
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response glu :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## type 1 45822 45822 59.014 7.075e-13 ***
## Residuals 198 153738 776
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response bp :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## type 1 1141.3 1141.28 9.009 0.003032 **
## Residuals 198 25083.2 126.68
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response skin :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## type 1 1569.2 1569.22 12.049 0.0006361 ***
## Residuals 198 25786.5 130.24
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response bmi :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## type 1 592.9 592.87 17.049 5.368e-05 ***
## Residuals 198 6885.4 34.77
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response age :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## type 1 3209.3 3209.3 30.606 9.926e-08 ***
## Residuals 198 20762.2 104.9
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Response ped :
## Df Sum Sq Mean Sq F value Pr(&gt;F)
## type 1 0.796 0.79600 8.7623 0.003451 **
## Residuals 198 17.987 0.09084
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>diab%&gt;%group_by(type)%&gt;%summarize(mean(npreg),mean(glu),mean(bp),mean(skin),mean(bmi),mean(age),mean(ped))</code></pre>
<pre><code>## # A tibble: 2 x 8
## type `mean(npreg)` `mean(glu)` `mean(bp)` `mean(skin)`
`mean(bmi)` `mean(age)` `mean(ped)`
## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 No 2.92 113.  69.5 27.2 31.1 29.2 0.415
## 2 Yes 4.84 145.  74.6 33.1 34.7 37.7 0.549</code></pre>
<pre class="r"><code>pairwise.t.test(diab$npreg,diab$type,
 p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  diab$npreg and diab$type 
## 
##     No   
## Yes 1e-04
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(diab$glu,diab$type,
 p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  diab$glu and diab$type 
## 
##     No     
## Yes 7.1e-13
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(diab$bp,diab$type,
 p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  diab$bp and diab$type 
## 
##     No   
## Yes 0.003
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(diab$skin,diab$type,
 p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  diab$skin and diab$type 
## 
##     No     
## Yes 0.00064
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(diab$bmi,diab$type,
 p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  diab$bmi and diab$type 
## 
##     No     
## Yes 5.4e-05
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(diab$age,diab$type,
 p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  diab$age and diab$type 
## 
##     No     
## Yes 9.9e-08
## 
## P value adjustment method: none</code></pre>
<pre class="r"><code>pairwise.t.test(diab$ped,diab$type,
 p.adj=&quot;none&quot;)</code></pre>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  diab$ped and diab$type 
## 
##     No    
## Yes 0.0035
## 
## P value adjustment method: none</code></pre>
<p>After compleating an unvaqriate ANOVA, and a post-hoc t test, it was found that all of the variables showed a significant mean difference between diabetic and non diabetic women. A total of 15 tests where done on the data set. The probability of at least one type I error occuring in this case is 0.0033. Through concidering this bonferroni correction, now the variable diabetes pedigree function is no longer significant. The assumptions made in order to conduct a MANOVA were rqndom samples, Homogeneity of within-group covariance matrices, Multivariate normality of DVs and others. As there are many assumptions that need to be met for MANOVA’s and ANOVA’s, it is not likely that they were all met.</p>
</div>
<div id="randomization-test" class="section level2">
<h2>Randomization test</h2>
<p>In order to get a better understanding of my data, I decided to conduct a randomization test which highlights the difference in mean plasma glucose concentration between women who have diabietes and those who dont. H0 : mean plasma glucose concentration is the same for women with diabetes vs. without diabetes HA : mean plasma glucose concentration is different for women with diabetes vs. without diabetes</p>
<pre class="r"><code>rand_dist&lt;-vector()
for(i in 1:5000){
new&lt;-data.frame(Glucose_Conc=sample(diab$glu),type=diab$type)
rand_dist[i]&lt;-mean(new[new$type==&quot;Yes&quot;,]$Glucose_Conc)-
 mean(new[new$type==&quot;No&quot;,]$Glucose_Conc)}
mean(rand_dist)*2 </code></pre>
<pre><code>## [1] 0.04032977</code></pre>
<pre class="r"><code>ggplot(diab,aes(glu,fill=type))+geom_histogram(bins=6.5)+facet_wrap(~type,ncol=2)+theme()</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-4-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>t.test(data=diab,glu~type)</code></pre>
<pre><code>##
## Welch Two Sample t-test
##
## data: glu by type
## t = -7.3856, df = 121.76, p-value = 2.081e-11
## alternative hypothesis: true difference in means is not
equal to 0
## 95 percent confidence interval:
## -40.51739 -23.38813
## sample estimates:
## mean in group No mean in group Yes
## 113.1061 145.0588</code></pre>
<p>Through observing the p-value calculated, the null hypothesis is rejected, There is a difference in the mean plasma glucose concentration for women with diabetes vs. without diabetes. This conclusion is also supported through the observation of the t-test, where the p-value was also less than 0.05.</p>
</div>
<div id="linear-regression" class="section level2">
<h2>Linear Regression</h2>
<pre class="r"><code>y &lt;- as.numeric(diab$type == &quot;Yes&quot;)
diab1 &lt;- cbind(diab, y)
diab1&lt;-diab1%&gt;%dplyr::select(-`type`)</code></pre>
<pre class="r"><code>diab1$npreg &lt;- diab1$npreg - mean(diab1$npreg)
diab1$glu &lt;- diab1$glu - mean(diab1$glu)
diab1$age &lt;- diab1$age - mean(diab1$age)

fit&lt;-lm(y~glu*npreg*age,data=diab1)
summary(fit)</code></pre>
<pre><code>##
## Call:
## lm(formula = y ~ glu * npreg * age, data = diab1)
##
## Residuals:
## Min 1Q Median 3Q Max
## -0.84016 -0.27709 -0.09307 0.31783 0.98315
##
## Coefficients:
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 3.510e-01 3.667e-02 9.571 &lt; 2e-16 ***
## glu 6.211e-03 1.212e-03 5.124 7.25e-07 ***
## npreg 1.505e-02 1.325e-02 1.135 0.2577
## age 7.959e-03 3.970e-03 2.005 0.0464 *
## glu:npreg -1.220e-05 4.156e-04 -0.029 0.9766
## glu:age -5.145e-05 1.026e-04 -0.502 0.6164
## npreg:age -2.005e-04 9.349e-04 -0.214 0.8304
## glu:npreg:age -4.898e-06 3.167e-05 -0.155 0.8773
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Residual standard error: 0.4089 on 192 degrees of
freedom
## Multiple R-squared: 0.2847, Adjusted R-squared: 0.2587
## F-statistic: 10.92 on 7 and 192 DF, p-value: 1.402e-11</code></pre>
<p>For the linear Regression, the intercept represents the y component (Diabetes). Through observing the coefficient estimates, it can be seen that there is a possitive correlation between diabetes, and the variables glu, npreg and age. This shows that as glu, npreg, and age increase the incidents of diabetes increase. However there is a negative correlation between the interaction of glu and npreg, glu and age, npreg and age, and the interaction of all three predictors.</p>
<pre class="r"><code>ggplot(diab1, aes(x=glu, y=age,group=y))+geom_point(aes(color=y))+
 geom_smooth(method=&quot;lm&quot;,formula=y~1,se=F,fullrange=T,aes(color=y))+
theme(legend.position=c(.9,.19))+xlab(&quot;&quot;)</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-7-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>library(sandwich)
library(lmtest)
plot(fit)</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-8-1.png" width="768" style="display: block; margin: auto;" /><img src="/Project2_files/figure-html/unnamed-chunk-8-2.png" width="768" style="display: block; margin: auto;" /><img src="/Project2_files/figure-html/unnamed-chunk-8-3.png" width="768" style="display: block; margin: auto;" /><img src="/Project2_files/figure-html/unnamed-chunk-8-4.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>bptest(fit)</code></pre>
<pre><code>## 
##  studentized Breusch-Pagan test
## 
## data:  fit
## BP = 26.855, df = 7, p-value = 0.000354</code></pre>
<p>From observing the plot of the function it can be stated that all assumptions are met.</p>
<pre class="r"><code>library(sandwich)
library(lmtest)
coeftest(fit, vcov=vcovHC(fit))</code></pre>
<pre><code>##
## t test of coefficients:
##
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 3.5102e-01 4.1660e-02 8.4258 8.322e-15 ***
## glu 6.2109e-03 1.2253e-03 5.0691 9.369e-07 ***
## npreg 1.5046e-02 1.5129e-02 0.9945 0.32124
## age 7.9592e-03 4.6717e-03 1.7037 0.09005 .
## glu:npreg -1.2197e-05 3.8921e-04 -0.0313 0.97503
## glu:age -5.1454e-05 9.5912e-05 -0.5365 0.59225
## npreg:age -2.0050e-04 1.2816e-03 -0.1564 0.87585
## glu:npreg:age -4.8980e-06 3.4950e-05 -0.1401 0.88869
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<pre class="r"><code>(sum((y-mean(y))^2)-sum(fit$residuals^2))/sum((y-mean(y))^2)</code></pre>
<pre><code>## [1] 0.2847362</code></pre>
<p>Through observing the robust standard errods, now only the gly variable gives significant results compared to the first model where age also gave significant results. There are an increase in the std. error for variables with a positive estimate coeficient, and a dectrease in the std. error for variables with a negative estimate coeficient. The model explains 28.4% of the in the outcome.</p>
<pre class="r"><code>fit1&lt;-lm(y~glu+npreg+age,data=diab1)
summary(fit1)</code></pre>
<pre><code>##
## Call:
## lm(formula = y ~ glu + npreg + age, data = diab1)
##
## Residuals:
## Min 1Q Median 3Q Max
## -0.92326 -0.26907 -0.09296 0.30301 0.97178
##
## Coefficients:
## Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 0.3400000 0.0286635 11.862 &lt; 2e-16 ***
## glu 0.0060721 0.0009672 6.278 2.16e-09 ***
## npreg 0.0145573 0.0106713 1.364 0.1741
## age 0.0071415 0.0034339 2.080 0.0389 *
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## Residual standard error: 0.4054 on 196 degrees of
freedom
## Multiple R-squared: 0.2824, Adjusted R-squared: 0.2714
## F-statistic: 25.71 on 3 and 196 DF, p-value: 4.549e-14</code></pre>
<pre class="r"><code>lrtest(fit,fit1)</code></pre>
<pre><code>## Likelihood ratio test
## 
## Model 1: y ~ glu * npreg * age
## Model 2: y ~ glu + npreg + age
##   #Df  LogLik Df  Chisq Pr(&gt;Chisq)
## 1   9 -100.84                     
## 2   5 -101.17 -4 0.6561     0.9566</code></pre>
<pre class="r"><code>lrtest(fit1)</code></pre>
<pre><code>## Likelihood ratio test
##
## Model 1: y ~ glu + npreg + age
## Model 2: y ~ 1
## #Df LogLik Df Chisq Pr(&gt;Chisq)
## 1 5 -101.17
## 2 2 -134.35 -3 66.365 2.561e-14 ***
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1</code></pre>
<p>The likelihood-ratio test here shows that the regression without interaction fits the model better than with interactions.</p>
</div>
<div id="bootstrap" class="section level2">
<h2>Bootstrap</h2>
<pre class="r"><code>samp_distn&lt;-replicate(5000, {
 diab_dat&lt;-diab_dat&lt;-diab1[sample(nrow(diab1),replace=TRUE),]
 fit&lt;-lm(y~glu*npreg*age,data=diab1)
 coef(fit)
})

samp_distn%&gt;%t%&gt;%as.data.frame%&gt;%summarize_all(sd)</code></pre>
<pre><code>## (Intercept) glu npreg age glu:npreg glu:age npreg:age
glu:npreg:age
## 1 0 0 0 0 0 0 0 0</code></pre>
</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic Regression</h2>
<pre class="r"><code>fit &lt;- glm(y~., data = diab1, family = &quot;binomial&quot;)
summary(fit)</code></pre>
<pre><code>##
## Call:
## glm(formula = y ~ ., family = &quot;binomial&quot;, data = diab1)
##
## Deviance Residuals:
## Min 1Q Median 3Q Max
## -1.9830 -0.6773 -0.3681 0.6439 2.3154
##
## Coefficients:
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -4.100771 1.633687 -2.510 0.01207 *
## npreg 0.103183 0.064694 1.595 0.11073
## glu 0.032117 0.006787 4.732 2.22e-06 ***
## bp -0.004768 0.018541 -0.257 0.79707
## skin -0.001917 0.022500 -0.085 0.93211
## bmi 0.083624 0.042827 1.953 0.05087 .
## ped 1.820410 0.665514 2.735 0.00623 **
## age 0.041184 0.022091 1.864 0.06228 .
## ---
## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1
&#39; &#39; 1
##
## (Dispersion parameter for binomial family taken to be 1)
##
## Null deviance: 256.41 on 199 degrees of freedom
## Residual deviance: 178.39 on 192 degrees of freedom
## AIC: 194.39
##
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>prob &lt;- predict(fit,type=&quot;response&quot;)      
class_diag(prob, diab1$y)</code></pre>
<pre><code>##     acc      sens      spec       ppv       auc
## 1 0.775 0.5735294 0.8787879 0.7090909 0.8502674</code></pre>
<pre class="r"><code>table(predict=as.numeric(prob&gt;.5),truth=diab1$y)%&gt;%addmargins</code></pre>
<pre><code>##        truth
## predict   0   1 Sum
##     0   116  29 145
##     1    16  39  55
##     Sum 132  68 200</code></pre>
<pre class="r"><code>library(plotROC)
prob &lt;- predict(fit,type=&quot;response&quot;)  
ROC1&lt;-ggplot(diab1)+geom_roc(aes(d=y,m=prob), n.cuts=0)
ROC1</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-12-1.png" width="768" style="display: block; margin: auto;" /></p>
<pre class="r"><code>odds&lt;-function(x)x/(1-x)
x&lt;-seq(0,1,by=.1)
cbind(x, odds=odds(x))%&gt;%round(4)</code></pre>
<pre><code>##         x   odds
##  [1,] 0.0 0.0000
##  [2,] 0.1 0.1111
##  [3,] 0.2 0.2500
##  [4,] 0.3 0.4286
##  [5,] 0.4 0.6667
##  [6,] 0.5 1.0000
##  [7,] 0.6 1.5000
##  [8,] 0.7 2.3333
##  [9,] 0.8 4.0000
## [10,] 0.9 9.0000
## [11,] 1.0    Inf</code></pre>
<pre class="r"><code>logit&lt;-function(x)log(odds(x))
cbind(x, odds=odds(x),logit=logit(x))%&gt;%round(4)</code></pre>
<pre><code>##         x   odds   logit
##  [1,] 0.0 0.0000    -Inf
##  [2,] 0.1 0.1111 -2.1972
##  [3,] 0.2 0.2500 -1.3863
##  [4,] 0.3 0.4286 -0.8473
##  [5,] 0.4 0.6667 -0.4055
##  [6,] 0.5 1.0000  0.0000
##  [7,] 0.6 1.5000  0.4055
##  [8,] 0.7 2.3333  0.8473
##  [9,] 0.8 4.0000  1.3863
## [10,] 0.9 9.0000  2.1972
## [11,] 1.0    Inf     Inf</code></pre>
<pre class="r"><code>diab1$logit&lt;-predict(fit) 
diab1$y&lt;-factor(diab1$y)
ggplot(diab1,aes(logit, fill=y))+geom_density(alpha=.3)+
  geom_vline(xintercept=0,lty=2)</code></pre>
<p><img src="/Project2_files/figure-html/unnamed-chunk-12-2.png" width="768" style="display: block; margin: auto;" /> Through obsering the coefficients estimates, there is a possitive correlation between age and instance of diabetes, as well as glu and instance of diabetes. The Accuracy, TPR, TNR, and PPV were found to be 0.77, 0.559, 0.8787, and 0.703. This shows a fairly strong fiting of the model. The AUC was also found to be 0.8224 which also shows that the model predics diabetes well overall.</p>
<pre class="r"><code>set.seed(1234)
k=10
data1&lt;-diab1[sample(nrow(diab1)),]
folds&lt;-cut(seq(1:nrow(diab1)),breaks=k,labels=FALSE)
diags&lt;-NULL
for(i in 1:k){
 train&lt;-data1[folds!=i,]
 test&lt;-data1[folds==i,]
 truth&lt;-test$y
 fit&lt;-glm(y~age + glu,data=train,family=&quot;binomial&quot;)
 probs&lt;-predict(fit,newdata = test,type=&quot;response&quot;)
 diags&lt;-rbind(diags,class_diag(probs,truth))
}
apply(diags,2,mean)</code></pre>
<pre><code>##       acc      sens      spec       ppv       auc 
## 0.7550000 0.5473413 0.8780342 0.6391667 0.8362432</code></pre>
<p>After compleating a 10-fodl CV, the out-of-samply accuracy was found to be 0.755, sensitivity was 0.5473, and AUC was 0.8362. As the AUC value increased, this shows that there was no overfitting in the data set, and that the model fits well.</p>
</div>
<div id="lasso" class="section level2">
<h2>Lasso</h2>
<pre class="r"><code>library(glmnet)
set.seed(1234)
diab1&lt;-diab1%&gt;%na.omit()
fit&lt;-glm(y~ -1 + ., data=diab1,family=&quot;binomial&quot;)

x&lt;-model.matrix(fit)
y&lt;-as.matrix(diab1$y)
cv&lt;-cv.glmnet(x,y,family=&quot;binomial&quot;)
lasso&lt;-glmnet(x,y,family=&quot;binomial&quot;,lambda=cv$lambda.1se)
coef(lasso)</code></pre>
<pre><code>## 9 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                     s0
## (Intercept) -0.2931986
## npreg        .        
## glu          .        
## bp           .        
## skin         .        
## bmi          .        
## ped          .        
## age          .        
## logit        0.4776578</code></pre>
<pre class="r"><code>truth &lt;- diab1$y 
lasso_prob &lt;- predict(lasso, newx=x, type=&quot;response&quot;)
table(predicted= lasso_prob &gt;0.5, truth = diab1$y)</code></pre>
<pre><code>##          truth
## predicted   0   1
##     FALSE 124  37
##     TRUE    8  31</code></pre>
<pre class="r"><code>class_diag(lasso_prob, diab1$y)</code></pre>
<pre><code>##     acc      sens      spec       ppv       auc
## 1 0.775 0.4558824 0.9393939 0.7948718 0.8502674</code></pre>
<pre class="r"><code>lasso_vars &lt;- rownames(coef(lasso))[which(coef(lasso)&gt;0)]
diab2 &lt;- x %&gt;% as.data.frame %&gt;%select(lasso_vars)%&gt;%mutate(y = diab1$y)

set.seed(1234)
k=10

data1&lt;-diab2[sample(nrow(diab1)),]
folds&lt;-cut(seq(1:nrow(diab1)),breaks=k,labels=FALSE)

diags&lt;-NULL
for(i in 1:k){
 train&lt;-data1[folds!=i,]
 test&lt;-data1[folds==i,]
 truth&lt;-test$y
 fit&lt;-glm(y~.,data=train,family=&quot;binomial&quot;)
 probs&lt;-predict(fit,newdata = test,type=&quot;response&quot;)
 preds&lt;- ifelse(probs &gt; 0.5, 1, 0)
 diags&lt;-rbind(diags,class_diag(probs,truth))
}
apply(diags,2,mean)</code></pre>
<pre><code>##       acc      sens      spec       ppv       auc 
## 0.7750000 0.5651587 0.8771252 0.6769841 0.8602532</code></pre>
<p>After running a LASSO regression, the variables that were retained were npreg, glu, bmi, ped and age. When comparing the model’s out-of-sample accuracy to that of your logistic regression in part 5 it can be seen that there is a decrease in the accuracy value.</p>
<p>…</p>
</div>
</div>
